{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6124b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3035d916",
   "metadata": {},
   "source": [
    "### Scrapping from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc048afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://en.wikipedia.org/wiki/Artificial_intelligence'\n",
    "html_page = requests.get(URL).text\n",
    "soup = BeautifulSoup(html_page, 'lxml')\n",
    "paraContent = soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbe7eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\n",
    "for para in paraContent:\n",
    "    paragraph += para.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a9d7069",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = re.sub(r'\\[[0-9a-zA-Z]*\\]', ' ', paragraph)\n",
    "paragraph = re.sub(r'\\s+', ' ', paragraph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91eedfc",
   "metadata": {},
   "source": [
    "### NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eabadac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daaeadfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05da2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e01ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = {}\n",
    "word_tokens = nltk.word_tokenize(paragraph)\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89f8f395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Frequencies\n",
    "maximum_frquency_word = max(word_frequencies.values())\n",
    "for word in word_frequencies.keys():\n",
    "    word_frequencies[word] = (word_frequencies[word]/maximum_frquency_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19038e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Score\n",
    "sentence_scores = {}\n",
    "for sentence in sentence_tokens:\n",
    "    for word in nltk.word_tokenize(sentence.lower()):\n",
    "        if word in word_frequencies.keys():\n",
    "            if(len(sentence.split(\" \")) < 30):\n",
    "                if sentence not in sentence_scores.keys():\n",
    "                    sentence_scores[sentence] = word_frequencies[word]\n",
    "                else:\n",
    "                    sentence_scores[sentence] += word_frequencies[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63fb4359",
   "metadata": {},
   "source": [
    "### Combine top sentences "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "707a17bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7be72f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = heapq.nlargest(25, sentence_scores, key=sentence_scores.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd12a44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for sentence in summary:\n",
    "    sentences.append(nltk.word_tokenize(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bdc34e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, USA and Vietnam. When given a small, static, and visible environment, this is easy; however, dynamic environments, such as (in endoscopy) the interior of a patient\\'s breathing body, pose a greater challenge. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and the ability to move and manipulate objects. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining \"interesting\" and actionable inferences from large databases), and other areas. The rise of HCAI is visible in topics such as explainable AI, transparency, audit trail, fairness, trustworthiness, and controllable systems. A simple \"neuron\" N accepts input from other neurons, each of which, when activated (or \"fired\"), casts a weighted \"vote\" for or against whether neuron N should itself activate. [citation needed] Many problems in AI (in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. Soft computing tools were developed in the 80s, such as neural networks, fuzzy systems, Grey system theory, evolutionary computation and many tools drawn from statistics or mathematical optimization. Robotics researchers, such as Rodney Brooks, rejected symbolic AI and focused on the basic engineering problems that would allow robots to move, survive, and learn their environment. Affective computing is an interdisciplinary umbrella that comprises systems which recognize, interpret, process, or simulate human feeling, emotion and mood. AI also draws upon computer science, psychology, linguistics, philosophy, and many other fields. \"Neats\" hope that intelligent behavior be described using simple, elegant principles (such as logic, optimization, or neural networks). However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began. A few examples are: energy storage, deepfakes, medical diagnosis, military logistics, or supply chain management. A representation of \"what exists\" is an ontology: the set of objects, relations, concepts, and properties formally described so that software agents can interpret them. The narrow focus allowed researchers to produce verifiable results, exploit more mathematical methods, and collaborate with other fields (such as statistics, economics and mathematics). Herbert Simon predicted, \"machines will be capable, within twenty years, of doing any work a man can do\". A superintelligence, hyperintelligence, or superhuman intelligence, is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind. Deep learning has drastically improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, image classification and others. Many researchers began to doubt that the symbolic approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis), wherein AI classifies the affects displayed by a videotaped subject. However, the symbolic approach failed dismally on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. Specialized languages for artificial intelligence have been developed, such as Lisp, Prolog, TensorFlow and many others.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e0bafa",
   "metadata": {},
   "source": [
    "### Using sentence similarity and picking the best from the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f90c805",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster.util import cosine_distance\n",
    "import numpy as np\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "027db199",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stop_words):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list((set(sent1+sent2)))\n",
    "    \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    for w in sent1:\n",
    "        if w not in stop_words:\n",
    "            vector1[all_words.index(w)] += 1\n",
    "    for w in sent2:\n",
    "        if w not in stop_words:\n",
    "            vector2[all_words.index(w)] += 1\n",
    "    \n",
    "    return 1-cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e57cc3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sim_matrix(sentences, stop_words):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if(idx1 == idx2):\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16bc0be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(sentences, stop_words, top_n=5):\n",
    "    summarized_text = []\n",
    "    sentence_similarity_matrix = gen_sim_matrix(sentences, stop_words)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    ranked_sentences = sorted(((scores[i], s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    for i in range(top_n):\n",
    "        summarized_text.append(\" \".join(ranked_sentences[i][1]))\n",
    "    print(\"Summary: \\n\", \" \".join(summarized_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6fa44143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      " Most EU member states had released national AI strategies , as had Canada , China , India , Japan , Mauritius , the Russian Federation , Saudi Arabia , United Arab Emirates , USA and Vietnam . The traditional goals of AI research include reasoning , knowledge representation , planning , learning , natural language processing , perception , and the ability to move and manipulate objects . When given a small , static , and visible environment , this is easy ; however , dynamic environments , such as ( in endoscopy ) the interior of a patient 's breathing body , pose a greater challenge . AI also draws upon computer science , psychology , linguistics , philosophy , and many other fields . Soft computing is a set of techniques , including genetic algorithms , fuzzy logic and neural networks , that are tolerant of imprecision , uncertainty , partial truth and approximation . A few examples are : energy storage , deepfakes , medical diagnosis , military logistics , or supply chain management . However , beginning with the collapse of the Lisp Machine market in 1987 , AI once again fell into disrepute , and a second , longer-lasting winter began . Affective computing is an interdisciplinary umbrella that comprises systems which recognize , interpret , process , or simulate human feeling , emotion and mood . [ citation needed ] Many problems in AI ( in reasoning , planning , learning , perception , and robotics ) require the agent to operate with incomplete or uncertain information . Specialized languages for artificial intelligence have been developed , such as Lisp , Prolog , TensorFlow and many others .\n"
     ]
    }
   ],
   "source": [
    "generate_summary(sentences, stop_words, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dc6250",
   "metadata": {},
   "source": [
    "## Combining all the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95e5c79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_data(URL):\n",
    "    html_page = requests.get(URL).text\n",
    "    soup = BeautifulSoup(html_page, 'lxml')\n",
    "    paraContent = soup.find_all('p')\n",
    "    paragraph = \"\"\n",
    "    for para in paraContent:\n",
    "        paragraph += para.text\n",
    "    paragraph = re.sub(r'\\[[0-9a-zA-Z]*\\]', ' ', paragraph)\n",
    "    paragraph = re.sub(r'\\s+', ' ', paragraph)\n",
    "    \n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d936c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_important_sentences(data):\n",
    "    sentence_tokens = nltk.sent_tokenize(data)\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    word_frequencies = {}\n",
    "    word_tokens = nltk.word_tokenize(data)\n",
    "    for word in word_tokens:\n",
    "        if word not in stop_words:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "                \n",
    "    # Weighted Frequencies\n",
    "    maximum_frquency_word = max(word_frequencies.values())\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frquency_word)\n",
    "    \n",
    "    # Sentence Score\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentence_tokens:\n",
    "        for word in nltk.word_tokenize(sentence.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if(len(sentence.split(\" \")) < 30):\n",
    "                    if sentence not in sentence_scores.keys():\n",
    "                        sentence_scores[sentence] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sentence] += word_frequencies[word]\n",
    "    \n",
    "    top_sentences = heapq.nlargest(25, sentence_scores, key=sentence_scores.get)\n",
    "    result = []\n",
    "    for sentence in top_sentences:\n",
    "        result.append(nltk.word_tokenize(sentence))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d2ea7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_similarity(sent1, sent2, stop_words):\n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    "    all_words = list((set(sent1+sent2)))\n",
    "    \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    "    for w in sent1:\n",
    "        if w not in stop_words:\n",
    "            vector1[all_words.index(w)] += 1\n",
    "    for w in sent2:\n",
    "        if w not in stop_words:\n",
    "            vector2[all_words.index(w)] += 1\n",
    "    \n",
    "    return 1-cosine_distance(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "34601a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_sim_matrix(sentences, stop_words):\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if(idx1 == idx2):\n",
    "                continue\n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "08eb6823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(data, stop_words, top_n=5):\n",
    "    sentences = get_important_sentences(data)\n",
    "    summarized_text = []\n",
    "    sentence_similarity_matrix = gen_sim_matrix(sentences, stop_words)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_matrix)\n",
    "    scores = nx.pagerank(sentence_similarity_graph)\n",
    "    ranked_sentences = sorted(((scores[i], s) for i,s in enumerate(sentences)), reverse=True)\n",
    "    for i in range(top_n):\n",
    "        summarized_text.append(\" \".join(ranked_sentences[i][1]))\n",
    "    summary = \" \".join(summarized_text)\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "881e1f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most EU member states had released national AI strategies , as had Canada , China , India , Japan , Mauritius , the Russian Federation , Saudi Arabia , United Arab Emirates , USA and Vietnam . The traditional goals of AI research include reasoning , knowledge representation , planning , learning , natural language processing , perception , and the ability to move and manipulate objects . When given a small , static , and visible environment , this is easy ; however , dynamic environments , such as ( in endoscopy ) the interior of a patient 's breathing body , pose a greater challenge . AI also draws upon computer science , psychology , linguistics , philosophy , and many other fields . Soft computing is a set of techniques , including genetic algorithms , fuzzy logic and neural networks , that are tolerant of imprecision , uncertainty , partial truth and approximation . A few examples are : energy storage , deepfakes , medical diagnosis , military logistics , or supply chain management . However , beginning with the collapse of the Lisp Machine market in 1987 , AI once again fell into disrepute , and a second , longer-lasting winter began . Affective computing is an interdisciplinary umbrella that comprises systems which recognize , interpret , process , or simulate human feeling , emotion and mood . [ citation needed ] Many problems in AI ( in reasoning , planning , learning , perception , and robotics ) require the agent to operate with incomplete or uncertain information . Specialized languages for artificial intelligence have been developed , such as Lisp , Prolog , TensorFlow and many others .\n"
     ]
    }
   ],
   "source": [
    "scraped_data = scrape_data('https://en.wikipedia.org/wiki/Artificial_intelligence')\n",
    "summary = generate_summary(scraped_data, stop_words, top_n=10)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
